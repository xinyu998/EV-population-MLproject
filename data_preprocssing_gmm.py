# -*- coding: utf-8 -*-
"""Data_PreprocssingGMM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PC12-Yhd12bYoX_1Jd1aRLSer0ODtJxl

## Import data
"""

# import dataset
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
import pandas as pd 
EV_popu_WA = pd.read_csv('https://raw.githubusercontent.com/zhangqc00/6880Data/main/Electric_Vehicle_Population_Data.csv')
charging_station_data = pd.read_csv('https://raw.githubusercontent.com/zhangqc00/6880Data/main/EV-Charging-Raw-Data.csv')

"""## Select labels for Electric Vehicle Population Dataset"""

# electric vehicle population dataset
# 'Vehicle Location' provides the registration location of the electric vehicle
# 'Model Year', 'Make', and 'Model' represent the electric vehicleâ€™s brand. 
# 'Electric Vehicle Type' - only keep Battery Electric Vehicle (BEV)
EV_popu_WA_data = EV_popu_WA[['Vehicle Location','Model Year', 'Make', 'Model', 'Electric Vehicle Type', 'Electric Range', 'Base MSRP']]

# remove Plug-in Hybrid Electric Vehicle due to low battery range.
EV_popu_WA_data = EV_popu_WA_data[EV_popu_WA_data['Electric Vehicle Type'] == 'Battery Electric Vehicle (BEV)']
EV_popu_WA_data = EV_popu_WA_data[['Vehicle Location','Model Year', 'Make', 'Model', 'Electric Range', 'Base MSRP']]

# drop the NaN value
EV_popu_WA_data = EV_popu_WA_data.dropna().reset_index(drop=True)

# add the longitudinal and lateral column (better analyze the vehicle location)
EV_popu_WA_data["longitudinal"] = pd.NaT
EV_popu_WA_data["lateral"] = pd.NaT
for i in range(len(EV_popu_WA_data)):
  EV_popu_WA_data['longitudinal'][i] = float(EV_popu_WA_data['Vehicle Location'][i][6:].split(" ", 1)[0].split("(", 1)[1])
  EV_popu_WA_data['lateral'][i] = float(EV_popu_WA_data['Vehicle Location'][i][6:].split(" ", 1)[1].split(")", 1)[0])

#remove the sample data outside the longitudinal or lateral limit of WA State
EV_popu_WA_data = EV_popu_WA_data[EV_popu_WA_data['longitudinal'] < (-117.0256)]
EV_popu_WA_data = EV_popu_WA_data[EV_popu_WA_data['lateral'] > (45.5821)].reset_index(drop=True)

EV_popu_WA_data.head(20)

value_test = EV_popu_WA_data['Electric Range']

"""## Select lables for Georgia Tech charging station"""

charging_station_data = charging_station_data[['Start Date', 'End Date', 'Charging Time (hh:mm:ss)', 'Energy (kWh)', 'GHG Savings (kg)','Gasoline Savings (gallons)','Fee']]
charging_station_data = charging_station_data.dropna().reset_index(drop=True)
charging_station_data.head(10)

"""## Data visualization"""

# check for the Electric Vehicle location
import plotly.graph_objects as go
import plotly.express as px
fig = go.Figure()
fig = px.scatter(EV_popu_WA_data, x='longitudinal', y='lateral')
fig.show()

"""## Data visualization layered onto Google Map of WA, USA

A Google Maps API key is necessary to be able to create a Google Map. In-depth details as to how this is achieved can be found at https://thedatafrog.com/en/articles/show-data-google-map-python/
"""

#there are a few packages that we may or may not need
!pip install --upgrade kaggle #Kaggle allows users to find and publish data sets, 
              #explore and build models in a web-based data-science environment,
              #which may not be needed, given that we are already able to collobarte via Colab
!pip install gmap #gmap is Google Maps' Python package for anything related to Google Maps,
              #from bokeh.plotting import gmap
!pip install bokeh  #we'll be using Bokeh, a Python package that can be used for data visualizations of all sorts!
              #gmap can also be installed by importting it from Bokeh as such:
              #from bokeh.plotting import gmap
from bokeh.io import output_notebook
output_notebook()

#quick sanity check
import numpy as np
print("np.shape(EV_popu_WA_data):", np.shape(EV_popu_WA_data))
lat = np.array(EV_popu_WA_data['lateral'])
print("\n lat:", lat)
lng = np.array(EV_popu_WA_data['longitudinal'])
print("\n lng:", lng)
print("\n np.size(lat) == np.size(lng)? ", np.size(lat)==np.size(lng))

#now we need to create an enviroment variable for our API username and key for the Google Map data source
import os
os.environ['GOOGLE_API_USERNAME'] = 'gmap_api' #the username can be whatever we want
os.environ['GOOGLE_API_KEY'] = 'AIzaSyCzy5o_S-LrSWvZLuTazLDzKLm5FA9y_jU' #the API key is issued at 
#https://developers.google.com/maps/documentation/javascript/get-api-key
#g00gLe-APi-KeY   is not my authentic API key

"""***Unless using a private `g00gLe-APi-KeY` of your own, I recommend  stopping here without compiling any additional lines of code (refreshing).***"""

!export gmap_api
#%env  #can be used to view the enviroment variables existing/created

#we are ready to plot!
from bokeh.models import ColumnDataSource
from bokeh.models import GMapOptions
from bokeh.io import show
from bokeh.plotting import gmap
import os 
import json
from google.colab import files
import io
#df = files.upload()

api_key = os.environ['GOOGLE_API_KEY'] #calling out environment variable!
#print("api_key: \n",api_key)

df = EV_popu_WA_data #df is our data frame
bokeh_width, bokeh_height = 800, 600 #of the plot
lat, lon = 47.2729926,-120.8822769 #center point of WA

#style  = pd.read_json('gmap_lakes_n_roads_customs.json')

def plot(lat, lng, zoom=7, map_type='terrain'): #styles=style can be added
    gmap_opt = GMapOptions(lat=lat, lng=lng, #styles=style, 
                                              #GMapOptions() initializes tool constraints 
            map_type=map_type, zoom=zoom)     #for the optional tools available on a Bokeh plot.
    p = gmap(api_key,  map_options=gmap_opt,  #gmap() centers our scatter plot with with Google Maps, 
                                              #using our specified center points.
            title='Data Visualization over WA, USA Terrain Map', 
            width=bokeh_width, height=bokeh_height)
    
    #define a column data source: 
    source = ColumnDataSource(df)
    #specify the x and y columns as strings, 
    #and declare to be the same as ColumnDataSource:
    center = p.circle('longitudinal', 'lateral', size=4, alpha=5, 
                      color='red', source=source)
    show(p)
    return p

print("calling plot() ...\n")
p = plot(lat, lon, map_type='terrain')

"""## K-Mean"""

#data=EV_popu_WA_data
#X=EV_popu_WA_data[['longitudinal','lateral']]

import warnings
warnings.filterwarnings("ignore")
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import numpy as np
import pandas as pd

data=pd.read_excel('https://github.com/xinyu998/WA-EV-population-ML-project/blob/main/EVprocessed_data.xlsx?raw=true')
X=data[['longitudinal','lateral']]

## K-mean Elbow method to find the best k value
from sklearn import metrics
from scipy.spatial.distance import cdist
distortions = []
inertias = []
mapping1 = {}
mapping2 = {}
K=range(2,12)
 
for k in K:
    # Building and fitting the model
    kmeanModel = KMeans(n_clusters=k).fit(X)
    kmeanModel.fit(X)
 
    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_,
                                        'euclidean'), axis=1)) / X.shape[0])
    inertias.append(kmeanModel.inertia_)
 
    mapping1[k] = sum(np.min(cdist(X, kmeanModel.cluster_centers_,
                                   'euclidean'), axis=1)) / X.shape[0]
    mapping2[k] = kmeanModel.inertia_
for key, val in mapping1.items():
    print(f'{key} : {val}')
plt.plot(K, distortions, 'bx-')
plt.xlabel('Values of K')
plt.ylabel('Distortion')
plt.title('The Elbow Method using Distortion')
plt.show()

kmeans_model=KMeans(init="k-means++",n_clusters=6)
kmeans_model.fit(X)
h=.02
#print(X.iloc[:,0].min())
# Plot the decision boundary
x_min, x_max = X.iloc[:, 0].min()-1, X.iloc[:, 0].max()+1
y_min, y_max = X.iloc[:, 1].min()-1, X.iloc[:, 1].max()+1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

#Obtain labels for each point in mesh. Use last trained model.
Z = kmeans_model.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure(figsize=(15,8))
plt.imshow(Z, interpolation="nearest", extent=(xx.min(), xx.max(), yy.min(), yy.max()),
           cmap='coolwarm', aspect="auto", origin="lower")

# Plot the projected data on 2D space
plt.plot(X.iloc[:, 0], X.iloc[:, 1], 'k.', markersize=2)

# Plot the centroids as a white X
centroids = kmeans_model.cluster_centers_
plt.scatter(centroids[:, 0], centroids[:, 1], marker="x", s=120, linewidths=5, color="w", zorder=10)

plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.show()

# get sample points from each cluster
data=pd.read_excel('https://github.com/xinyu998/WA-EV-population-ML-project/blob/main/EVprocessed_data.xlsx?raw=true')

"""**Plot of Each Clusters**"""

#i th cluster 

## get sample points from each cluster
cluster_map=pd.DataFrame()
cluster_map['data_index'] = data.index.values
cluster_map['cluster'] = kmeans_model.labels_
cluster_map['longitudinal']=data.longitudinal
cluster_map['lateral']=data.lateral
cluster_map['year']=data.ModelYear
cluster_map['make']=data.Make
cluster_map['range']=data.ElectricRange
#cluster_map.to_excel('cluster3.xlsx')

fig, axs = plt.subplots(2,3)
for i,axs in zip(range (0,6),axs.ravel()):
    cluster= cluster_map.loc[cluster_map['cluster']==i]
    x_min, x_max = cluster.iloc[:,2].min()-0.2, cluster.iloc[:,2].max()+0.2
    y_min, y_max = cluster.iloc[:,3].min()-0.2, cluster.iloc[:,3].max()+0.2
    axs.plot(cluster.iloc[:,2], cluster.iloc[:, 3],'k.',markersize=2)
    axs.set_xlim(x_min, x_max)
    axs.set_ylim(y_min, y_max)
    axs.set_title('cluster '+str([i]))
plt.show()

"""**Shape of Clusters**"""

for i in range (0,6):
  cluster= cluster_map.loc[cluster_map['cluster']==i]
  print("cluster_"+str(i)+"_shape: ", cluster.shape)

"""**Data Set of Clusters**

Looking at the clusters below, we can clearly see that that each vehicles as been indexed with a unique value! We can use this index to match each EV with their corresponding range.
"""

cluster_0 = cluster_map.loc[cluster_map['cluster']==0]
cluster_1 = cluster_map.loc[cluster_map['cluster']==1]
cluster_2 = cluster_map.loc[cluster_map['cluster']==2]
cluster_3 = cluster_map.loc[cluster_map['cluster']==3]
cluster_4 = cluster_map.loc[cluster_map['cluster']==4]
cluster_5 = cluster_map.loc[cluster_map['cluster']==5]

cluster_0.head()

cluster_1.head()

cluster_2.head()

cluster_3.head()

cluster_4.head()

cluster_5.head()

"""**Indexed-EV to Range Data Set of Clusters**

We have K=10 mean clusters, assuming we stoke with this cluster amount, we would need ten datasets of the form: EV vehicles in n cluster vs their range, where n = 0, 1, ... 9  therefore, we need to create ten, np.size(EV vehicles in n cluster) by 1(their range) arrays ten clusters EV vs Range cluster the form 
        clus_0 = [[0:0], [1:0], ...[np.size(n_EV_at_clus_0):0]
        
        clus_1 = [[0:0], [1:0], ...[np.size(n_EV_at_clus_1):0]
        clus_2 = [[0:0], [1:0], ...[np.size(n_EV_at_clus_2):0]
        .
        .
        .
        clus_9 = [[0:0], [1:0], ...[np.size(n_EV_at_clus_9)::0]

These would be the dataset used to perform SVR training and predict the mean range of EVs at each cluster
"""

cluster_5_range = np.array(cluster_5['range'])
print("cluster_5_range size:", np.size(cluster_5_range))
cluster_5_EV_index = np.array(cluster_5['data_index'])
print("\n cluster_5_EV_index size:", np.size(cluster_5_EV_index))

plt.scatter(cluster_5_EV_index, cluster_5_range,c='blue')
plt.xlabel('EV_index')
plt.ylabel('range')
plt.title('cluster_5 EV index vs range')
plt.show()

cluster_5_range = np.array(cluster_5['range'])
cluster_5_EV_index = np.array(cluster_5['data_index'])

print("cluster_5_EV_index size:", np.size(cluster_5_EV_index))

plt.scatter(cluster_5_EV_index, cluster_5_range,c='blue')
plt.xlabel('EV_index')
plt.ylabel('range')
plt.title('cluster_5 EV index vs range')
plt.show()

range_index_0 = pd.DataFrame()
range_index_0['index']=cluster_0.index.values
for i in range (0,6):
    cluster= cluster_map.loc[cluster_map['cluster']==i]
    EVrange=cluster.iloc[:,5]
    print(np.size(data.index.values))
range_0 = []
i = 0
while i < np.size(data.index.values):
    if data.index.values[i] in cluster_0.index.values:
       range_index_0['range'] = data.ElectricRange.values[i]
       range_0.append(data.ElectricRange.values[i])
    i+=1
print("range_index_0.head()")
range_index_0.head()

range = np.array(EV_popu_WA_data['range'])

print("data.index.values:", data.index.values)

print("data.index.values:", cluster_0.index.values)

"""## Gaussian Mixture Model I

Weaknesses of k-Means: 
k-means finds suitable clustering results for well-separated data. For example, if we have simple blobs of data, the k-means algorithm can quickly label those clusters in a way that closely matches what we might do by eye. 

A Gaussian mixture model (GMM) attempts to find a mixture of multi-dimensional Gaussian probability distributions that best model any input dataset.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
GM_data = EV_popu_WA_data[['longitudinal','lateral']]
GM_data.head(10)

# training gaussian mixture model 
from sklearn.mixture import GaussianMixture
gmm = GaussianMixture(n_components=6)
gmm.fit(GM_data)
#predictions from gmm
labels = gmm.predict(GM_data)
frame = pd.DataFrame(GM_data)
frame['cluster'] = labels
frame.columns = ['longitudinal', 'lateral', 'cluster']

model11 = gmm.fit(GM_data)
model11.means_
long_list = []
lat_list = []
for i in range(len(model11.means_)):
  long_list.append(model11.means_[i][0])
  lat_list.append(model11.means_[i][1])


plt.scatter(long_list,lat_list,c='blue')
plt.xlabel('longitudinal')
plt.ylabel('lateral')
plt.title('Mean location of each cluster')
plt.show()

color=['blue','green','cyan', 'black','red','magenta']
plt.figure(figsize=(15,7))
for k in range(0,6):
    data = frame[frame["cluster"]==k]
    plt.scatter(data['longitudinal'],data['lateral'],c=color[k])
plt.xlabel('longitudinal')
plt.ylabel('lateral')
plt.title('Data Distribution')
plt.show()

data_k1 = frame[frame["cluster"]==0]
data_k2 = frame[frame["cluster"]==1]
data_k3 = frame[frame["cluster"]==2]
data_k4 = frame[frame["cluster"]==3]
data_k5 = frame[frame["cluster"]==4]
data_k6 = frame[frame["cluster"]==5]

# training gaussian mixture model for each clusters
from sklearn.mixture import GaussianMixture
location = []
long_list = []
lat_list = []

long_list.append(model11.means_[i][0])
lat_list.append(model11.means_[i][1])

gmm = GaussianMixture(n_components=5)
model_k1 = gmm.fit(data_k1)
for i in range(5):
  long_list.append(model_k1.means_[i][0])
  lat_list.append(model_k1.means_[i][1])

model_k2 = gmm.fit(data_k2)
location.append(model_k2.means_)
for i in range(5):
  long_list.append(model_k2.means_[i][0])
  lat_list.append(model_k2.means_[i][1])

model_k3 = gmm.fit(data_k3)
location.append(model_k3.means_)
for i in range(5):
  long_list.append(model_k3.means_[i][0])
  lat_list.append(model_k3.means_[i][1])

model_k4 = gmm.fit(data_k4)
location.append(model_k4.means_)
for i in range(5):
  long_list.append(model_k4.means_[i][0])
  lat_list.append(model_k4.means_[i][1])

model_k5 = gmm.fit(data_k5)
location.append(model_k5.means_)
for i in range(5):
  long_list.append(model_k5.means_[i][0])
  lat_list.append(model_k5.means_[i][1])

model_k6 = gmm.fit(data_k6)
location.append(model_k6.means_)
for i in range(5):
  long_list.append(model_k6.means_[i][0])
  lat_list.append(model_k6.means_[i][1])

plt.scatter(long_list,lat_list,c='blue')
plt.xlabel('longitudinal')
plt.ylabel('lateral')
plt.title('Location of each charing station')
plt.show()

location

# training gaussian mixture model 
from sklearn.mixture import GaussianMixture
gmm = GaussianMixture(n_components=6)
gmm.fit(GM_data)
#predictions from gmm
labels = gmm.predict(GM_data)
frame = pd.DataFrame(GM_data)
frame['cluster'] = labels
frame.columns = ['longitudinal', 'lateral', 'cluster']
for k in range(0,6):
    data = frame[frame["cluster"]==k]
    plt.scatter(data['longitudinal'],data['lateral'],c=color[k])
plt.xlabel('longitudinal')
plt.ylabel('lateral')
plt.title('Data Distribution')
plt.show()

from sklearn import datasets
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from yellowbrick.cluster import SilhouetteVisualizer
#
# Load the IRIS dataset
#
X = GM_data
  
fig, ax = plt.subplots(3, 2, figsize=(15,8))
for i in [2, 3, 4, 5, 6, 7, 8, 9]:
    '''
    Create KMeans instance for different number of clusters
    '''
    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)
    q, mod = divmod(i, 2)
    '''
    Create SilhouetteVisualizer instance with KMeans instance
    Fit the visualizer
    '''
    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])
    visualizer.fit(X)

"""## Gaussian Mixture Model II"""

from sklearn.mixture import GaussianMixture # for GMM clustering
from sklearn import metrics # for calculating Silhouette score

import matplotlib.pyplot as plt # for data visualization
import plotly.express as px  # for data visualization
import plotly.graph_objects as go # for data visualization

from geopy.geocoders import Nominatim # for getting city coordinates
from progressbar import ProgressBar # for displaying progress 
import time # for adding time delays

import plotly.graph_objects as go # for data visualization
df_loc = GM_data
# Create a figure
fig = go.Figure(data=go.Scattergeo(
        lat=df_loc['lateral'],
        lon=df_loc['longitudinal'],
        #hovertext=df_loc['Loc'], 
        mode = 'markers',
        marker_color = 'black',
        ))

# Update layout so we can zoom in on Australia
fig.update_layout(
        width=980,
        height=720,
        margin={"r":0,"t":10,"l":0,"b":10},
        geo = dict(
            scope='world',
            projection_type='miller',
            landcolor = "rgb(250, 250, 250)",
            center=dict(lat=47, lon=-122), # focus point
            projection_scale=35 # zoom in on
        ),
    )
fig.show()

# Create empty list
S=[]

# Range of clusters to try (2 to 10)
K=range(2,11)

# Select data for clustering model
X = df_loc[['lateral', 'longitudinal']]

for k in K:
    # Set the model and its parameters
    model = GaussianMixture(n_components=k, n_init=20, init_params='kmeans')
    # Fit the model 
    labels = model.fit_predict(X)
    # Calculate Silhoutte Score and append to a list
    S.append(metrics.silhouette_score(X, labels, metric='euclidean'))

# Plot the resulting Silhouette scores on a graph
plt.figure(figsize=(16,8), dpi=300)
plt.plot(K, S, 'bo-', color='black')
plt.xlabel('k')
plt.ylabel('Silhouette Score')
plt.title('Identify the number of clusters using Silhouette Score')
plt.show()

X = df_loc[['lateral', 'longitudinal']]

# Set the model and its parameters - 6 clusters
model6 = GaussianMixture(n_components=6, # this is the number of clusters
                         covariance_type='full', # {â€˜fullâ€™, â€˜tiedâ€™, â€˜diagâ€™, â€˜sphericalâ€™}, default=â€™fullâ€™
                         max_iter=100, # the number of EM iterations to perform. default=100
                         n_init=1, # the number of initializations to perform. default = 1
                         init_params='kmeans', # the method used to initialize the weights, the means and the precisions. {'random' or default='k-means'}
                         verbose=0, # default 0, {0,1,2}
                         random_state=1 # for reproducibility
                        )

# Fit the model and predict labels
clust6 = model6.fit(X)
labels6 = model6.predict(X)

# Generate 10,000 new samples based on the model
smpl=model6.sample(n_samples=10000)

# Print model summary
print('*************** 6 Cluster Model ***************')
print('Means: ', clust6.means_)
print('Converged: ', clust6.converged_)
print(' No. of Iterations: ', clust6.n_iter_)

# Attach cluster labels to the main dataframe
df_loc['Clust6']=labels6

# Create a figure
fig = go.Figure(data=go.Scattergeo(
        lat=df_loc['lateral'],
        lon=df_loc['longitudinal'],
        hovertext=df_loc[['Clust6']], 
        mode = 'markers',
        marker=dict(colorscale=['blue','green','cyan', 'black','red','magenta']),
        marker_color = df_loc['Clust6'],
        ))

# Update layout so we can zoom in on Australia
fig.update_layout(
        showlegend=False,
        width=1000,
        height=760,
        margin={"r":0,"t":30,"l":0,"b":10},
        geo = dict(
            scope='world',
            projection_type='miller',
            landcolor = "rgb(250, 250, 250)",
            center=dict(lat=47, lon=-122), # focus point
            projection_scale=35 # zoom in on
        ),
    )
fig.show()

"""Predicting the mean-EV range for each Small K-Means clusters"""

from sklearn.svm import SVR

#we are ready to plot!
from bokeh.models import ColumnDataSource
from bokeh.models import GMapOptions
from bokeh.io import show
from bokeh.plotting import gmap
import os 
import json
from google.colab import files
import io
#df = files.upload()

api_key = os.environ['GOOGLE_API_KEY'] #calling out environment variable!
#print("api_key: \n",api_key)

df = EV_popu_WA_data #df is our data frame
bokeh_width, bokeh_height = 800, 600 #of the plot
lat, lon = 47.2729926,-120.8822769 #center point of WA

#style  = pd.read_json('gmap_lakes_n_roads_customs.json')

def plot(lat, lng, zoom=7, map_type='terrain'): #styles=style can be added
    gmap_opt = GMapOptions(lat=lat, lng=lng, #styles=style, 
                                              #GMapOptions() initializes tool constraints 
            map_type=map_type, zoom=zoom)     #for the optional tools available on a Bokeh plot.
    p = gmap(api_key,  map_options=gmap_opt,  #gmap() centers our scatter plot with with Google Maps, 
                                              #using our specified center points.
            title='Data Visualization over WA, USA Terrain Map', 
            width=bokeh_width, height=bokeh_height)
    
    #define a column data source: 
    source = ColumnDataSource(df)
    #specify the x and y columns as strings, 
    #and declare to be the same as ColumnDataSource:
    center = p.circle('longitudinal', 'lateral', size=4, alpha=5, 
                      color='red', source=source)
    show(p)
    return p

print("calling plot() ...\n")
p = plot(lat, lon, map_type='terrain')

"""##Support Vector Regression"""